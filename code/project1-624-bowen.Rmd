---
title: 'DATA 624: Project 1'
author: "Andrew Bowen"
date: "2024-03-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(feasts)
library(fpp3)
library(fabletools)
library(readxl)
library(tsibble)
library(tseries)
```


# Part A
In this part, we'll be attempting to forecast how much cash is taken out of 4 different ATM machines for May 2010. Our dataset is provided for us, and available on my GitHub. 

## Data Wrangling and Visualization

Our `DATE` column looks to be the number of days since 1900, so we 

```{r}
# Read in our ATM data
atm <- read_excel("../data/ATM624Data.xlsx")

# Store ATM data in a tsibble object
atm <- atm %>%  
  mutate(DATE = as.Date(DATE, origin = "1899-12-30")) %>% as_tsibble(index=DATE, key=ATM) %>% fill_gaps()
```


To start off with, let's plot our `atm` tsibble to get a sense of the data. We see that we have data for 4 ATMs, with some `NA` values for 
```{r plot-atm, warning=FALSE, message=FALSE}
atm %>% autoplot() + labs(x = "Date", y="Cash Withdrawn (Hundred of Dollars)", title="Cash withdrawn from ATMs 1-4: May 2009 - May 2010")
```

In our case, the `NA` values for the `ATM` variable all occur in May 2010, which is the month we are trying to predict. In this case, we should be able to drop these rows for now, with the intention of "re-adding" them later when we forecast
```{r filter-atm-na}
# Filter null ATM rows for later forecasting replacement
atm <- atm %>% filter(!is.na(ATM))
```


```{r plot-atm-no-na, warning=FALSE, message=FALSE}
atm %>% autoplot() + labs(x = "Date", y="Cash Withdrawn (Hundred of Dollars)", title="Cash withdrawn from ATMs 1-4: May 2009 - May 2010")
```
## Imputation
As a result of our `fill_gaps` call above, we're left with some NA values in our time series. We can find the mean cash withdrawn per ATM varies between the machines

```{r atm-mean}
mean_cash_by_atm <- atm %>% 
     filter(!is.na(Cash)) %>%
     as_tibble %>% 
     group_by(ATM) %>% 
     summarise(Cash = mean(Cash))
mean_cash_by_atm
```

As such, it makes more sense to impute *per ATM*, rather than taking the blanket mean of our `Cash` variable.

```{r atm-imputation}
# Impute with mean value
atm <- atm %>% group_by(ATM) %>%
  mutate(Cash = ifelse(is.na(Cash), 
                       mean(Cash, na.rm=TRUE), 
                       Cash)) %>%
  ungroup()

atm %>% autoplot()
```

## Outliers
We see a strong outlier point around February 2010, in which it appears a value of $\$1,092,000$ was withdrawn. There is a chance that this outlier is due to an internal error within the data collection process, so we'll have to handle that as well. For our purposes, we can replace that outlier value with

```{r fix-atm-outliers}
atm_with_outlier <- atm # Save TS with outlier for later comparison
outlier_limit <- 3000
atm_outlier <- atm %>% filter(Cash > outlier_limit) %>% mutate(Cash = mean(atm$Cash))

# Remove outlier row and add in "impoted" value
atm <- atm %>% filter(Cash < outlier_limit)

atm <- bind_rows(atm, atm_outlier)

# Plot 
atm %>% autoplot()

```

Finally, let's plot our time series *without* `ATM4`, which tends to drown out the other ATM withdrawal values in terms of scale

```{r plot-other-outliers}
atm %>% filter(ATM != "ATM4") %>% autoplot(Cash)
```


## Decomposition
These time series appear to not have a strong overall trend upwards or downwards. However, there does appear to be a seasonal component. In our case, the seasonal variation **does not** change with time, so an additive decomposition may be better than a multiplicative one:
```{r atm-decomp, warning=FALSE, message=FALSE}
atm  %>% model(
    classical_decomposition(Cash, type = "additive")
  ) %>%
  components() %>%
  autoplot() +
  labs(title = "Classical Decomposition fo ATM data")
```

## Modeling

Now we need to get to actually build a model to predict the amount of cash taken out of each ATM for May 2010. One consideration if we need to build an ARIMA model would be to determine if our time series is stationary. We can do this via the [Augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickeyâ€“Fuller_test), with a null hypothesis that our data is **non-stationary** and a significance level $\alpha=0.05$. We can leverage the `adf.test` function from the R `tseries` library.

```{r}
adf.test(atm$Cash)
```
In this case, we see a p-value of 0.01, so we can reject the null hypothesis and assume our time series is stationary. This means we **won't** have to apply differencing to our data, since it's already stationary.

Let's also take a look at the ACF plot of our data. We see significant weekly lags (7 days) with autocorrelation. In simpler terms, ATM users tend to withdraw similar amounts depending on the day of the week.

```{r atm-acf}
# Plot the ACF of our ATM time series
atm %>% ACF(Cash) %>% autoplot()
```
We can also look at a lag plot for our data. Here we see a similar pattern from our ACF plot, with a strong autocorrelation at a lag of 7 (weekly).
```{r}
atm %>% filter(ATM == "ATM1") %>% gg_lag(geom="point")
```
Now we can fit an ARIMA model to the ATM dataset. When using ARIMA in fable, the default behavior is that model parameters are automatically selected. 
```{r atm-arima}
atm_fit <-atm %>% model(ARIMA(Cash))

atm_fit
```

Our `mable` for the ATM dataset contains an ARIMA model per panel (in this case, per ATM series). Each ARIMA model contains different values for $p, q, d$ as parameters. 

```{r atm-arima-forecast}
# Forecast and plot
atm_fc <- atm_fit %>% forecast(h=10)

# Plot forecast per ATM
atm_fc %>% autoplot(atm) + labs(x="Date",
                                y="Cash Withdrawn",
                                title="ARIMA models applied to ATM Withdrawal Datasets")
```

Our forecasts look reasonable visually, but we'll 


Finally, we can write our output dataset to a CSV which can be read into excel. The output can be found on my [GitHub here](https://github.com/andrewbowen19/predictiveAnalyticsData624/blob/main/data/atm-predictions.csv)
```{r}
# Write predictions to CSV
atm_fc %>% dplyr::select("ATM", "DATE", "Cash", ".mean") %>%
  dplyr::rename( "prediction" =".mean") %>%
  write_excel_csv(file="../data/atm-predictions.csv")
```


# Part B
In this part we'll be looking at residential power usage between 1998 and 2010. First, we'll read in the datasets used. and perform some basic wrangling to get into a 
```{r power-read-in}
# Read in our Power Usage data
usage <- read_excel("../data/ResidentialCustomerForecastLoad-624.xlsx")

# Store ATM data in a tsibble object
usage %>% 
     rename(Date = `YYYY-MMM`) %>% 
     mutate(Date = yearmonth(Date))
```




# Part C


