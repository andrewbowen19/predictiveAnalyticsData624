---
title: 'DATA 624: Homework 4'
author: "Andrew Bowen"
date: "2024-02-17"
output: html_document
---



```{r libraries, message=FALSE, echo=FALSE}
library(tidyverse)
library(fpp3)
library(fabletools)
library(mlbench)
library(tsibble)
library(corrplot)
library(GGally)
library(nnet)
library(MASS)
library(AppliedPredictiveModeling)
library(caret)
library(questionr)
library(gridExtra)
library(mice)
library(vcd)
```

## Exercise 3.1 (*K&J*)

```{r}
data(Glass)
head(Glass)
```

We can view the relationship between variables and our predictor (`Type`) via a correlation plot from the `corrplot` package in R
```{r}
# First, calculate the correlatino between variables in 
glass <- Glass %>% mutate(Type = as.numeric(Type))
corrGlass <- cor(glass)
corrplot(corrGlass)
```
We see reasonably strong correlations between our `Type` variable and the predictors for percentage of Barium, Alimunum, Magnesium, and Sodium.

We can also use a corner plot from the `GGally::ggpairs` function to visually inspect the relationships between our variables. We can also gain a sense of the distributions of each variable from this plot. I'm going to remove the correlations (`upper=NULL`; see above) to make the plot a bit less busy:
```{r warning=FALSE, message=FALSE}
# Use GGally to plot pairwise relationships between variables
ggpairs(glass, upper=NULL)
```

The predictor variables for the minerals we care about from above seem to be a bit skewed from this visual. Specifically, the below predictor variables seem to be significantly skewed such that a transformation is necessary:

- `RI`
- `Mg`
- `K`
- `Ca`
- `Ba`
- `Fe`

Some of which have strong correlations with our `Type` variable (see above)

There seem to be a few outlier points for the potassium (`K`) predictor, but this variable doesn't correlate strongly with our outcome variable.

#### Transformations
Let's build a simple logistic regression model to classify `Type`. First, we'll do it with the raw data, adn then with the transformed data to see if that improves our prediction. We'll [build a simple train-test splitas well](https://stackoverflow.com/questions/17200114/how-to-split-data-into-training-testing-sets-using-sample-function) to evaluate the models
```{r}
# Create simple training and test datasets
set.seed(1234) # Set Seed so that same sample can be reproduced in future also
# Now Selecting 75% of data as sample from total 'n' rows of the data  
sample <- sample.int(n = nrow(glass), size = floor(.75 * nrow(glass)), replace = F)
train <- glass[sample, ]
test  <- glass[-sample, ]

raw_model <- multinom(Type ~.,  data=train)
summary(raw_model)
```

Now we can apply some transforms to our predictors. We'll try a [*spatial sign transformation*](https://topepo.github.io/caret/pre-processing.html), which helps to reduce the effect of outliers by mapping our predictor values onto the surface of a hyper-dimensional sphere. Since all transformed data points will be the same distance from the center of the sphere, their effect as an outlier will be reduced.

In addition, our `Ca` variable is collinear with some other predictor variables, so we can remove it, as the variance in our class labels is likely captured by those additional predictors outside of Calcium.
```{r}
# First we scale our dataset then input it into the spatialSign transform
glass_predictors <- glass %>% dplyr::select(-c(Type))
# glass_predictors$Mg <- log(glass_predictors$Mg)


glass_scaled <- data.frame(scale(glass_predictors))
glass_transformed <- as.data.frame(spatialSign(glass_scaled))

glass_transformed <- glass_transformed %>% cbind(Type=glass$Type)

# Remove Calcium and Silicon from dataset
glass_transformed <- glass_transformed %>% dplyr::select(-c(Ca))
```



```{r}
# Now Selecting 75% of data as sample from total 'n' rows of the data  
sample <- sample.int(n = nrow(glass_transformed), size = floor(.75 * nrow(glass_transformed)), replace = F)
train_transformed <- glass_transformed[sample, ]
test_transformed  <- glass_transformed[-sample, ]

transformed_model <- multinom(Type ~.,  data=train_transformed)#, MaxNWts=1450)
```

```{r}
summary(transformed_model)
```

With our predictor variables transformed via spatial sign we see an improved (lower) AIC, indicating a simplaer model (this makes sense as we also removed some extraneous predictors). 

```{r}
raw_predict <- predict(raw_model, test)
transformed_predict <- predict(transformed_model, test_transformed)

# Calculate and print accuracy on raw data model and transformed data model
(raw_accuracy <- mean(raw_predict == test$Type))
(transformed_accuracy <- mean(transformed_predict == test_transformed$Type))
```

We see comparable accuracy numbers out of a simpler model (fewer inputs; lower AIC value) for our transformed data. 

## Exercise 3.2 (*K&J*)

```{r load-soybean}
data(Soybean)
head(Soybean)
```

## Categorical Predictor Frequency Distributions

The `table` function can be a helpful way to show the relative frequencies between categorical variables.
```{r}
table(Soybean$Class, Soybean$precip)
```


However, we have 36 variables in our `Soybean` dataset, so that table would become rather busy quickly. We can produce some `mosaic` plots form the VCD library which can help us to visualize some of our class frequencies. Lets do this first for some features related to the weather a soyeban experienced, for example: `twemp`, `hail`, and `precip`
```{r}
weather <- Soybean %>% dplyr::select(precip, hail, temp) %>% table()
mosaic(weather, shade = TRUE)
```

We can also plot some variables related to properties of the seed (e.g., `seed`, `seed.discolor`, `seed.size`). In this case, we see some larger class imbalance, specifically for the `see.discolor` and `seed.size` variables
```{r}
Soybean %>% dplyr::select(seed, seed.discolor, seed.size) %>% 
  table() %>%
  mosaic(shade = TRUE)
```


### Missing Values

To see which predictors have higher rates of missing values, we can use the `freq.na` fucntions fromt he `questionr` package, which lists the number and percentage of missing values per feature
```{r}
freq.na(Soybean)
```

From this it looks like we have several variables that are missing roughly a fifth of values (`hail`, `sever`, `seed.tmt`, `lodging` and others).


```{r}
# Pick out just predictors with many missing vals
soybean_missing <- Soybean %>% dplyr::select("Class", "sever", "hail", "seed.tmt", "lodging")

p1 <- ggplot(soybean_missing, aes(x=Class, fill=sever)) +
     geom_bar(position="fill") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(title = "Severing")

p2 <- ggplot(soybean_missing, aes(x=Class, fill=seed.tmt)) +
     geom_bar(position="fill") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ labs(title = "Seed Treatment")

p3 <- ggplot(soybean_missing, aes(x=Class, fill=hail)) +
     geom_bar(position="fill") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(title = "Received Hail?")

p4 <- ggplot(soybean_missing, aes(x=Class, fill=lodging)) +
     geom_bar(position="fill")  + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(title = "Raised in Lodging?")
```


```{r, sever}
p1
```

There's better balance for our treatment variable, though some classes are missing predictor values entirely
```{r, seed-treatment}
p2
```

For our `lodging` property, we see some stronger class imbalance, as well as some completely null class values
```{r, hail}
p3
```

```{r, lodging}
p4
```

Looking at the above plots, the predictors are missing values completely for 4 values for Soybean `Class`:

- `2-4-d-injury`
- `cyst-nematode`
- `diaporthe-&-stem-blight`
- `herbicide-injury`


### Missing Values Strategy
To handle these missing predictors, we could use the [`mice` package in R to impute values](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjel5L_p8CEAxV1jokEHUr_DhwQFnoECAYQAQ&url=https%3A%2F%2Fcran.r-project.org%2Fpackage%3Dmice&usg=AOvVaw3teMSC5Ocn4Mr47XlZeBHq&opi=89978449). Another method would be to sample from a distribution

Eliminating predictors is a simpler approach, in which we drop the pre
```{r}
soybean_eliminated <- Soybean %>% dplyr::select(-c("sever", "hail", "seed.tmt", "lodging"))
```


We can train a basic Multinomial Logistic Regression model with our "bad" predictors eliminated
```{r}
elim_model <- multinom(Class ~ ., soybean_eliminated)

# Print AIC which is a good measure of model complexity and goodness of fit
elim_model$AIC
```

Now we can try to train another multinomial logistic regression model on imputed data to compare. We'll only imput our biggest offender predictors from above to reduce compute complexity
```{r impute-soybean, warning=FALSE, message=FALSE}
soybean_missing <- soybean_missing %>% dplyr::select(-c(Class))
# Impute using mice
soybean_missing_imputed <- mice(soybean_missing)
```


```{r}
# Combine imputed and missing data
soybean_imputed <- cbind(soybean_eliminated, soybean_missing_imputed$data)

# Now we can train a Multinom Logistic Regression model on our imputed data
impute_model <- multinom(Class ~ ., soybean_imputed)

# Print AIC which is a good measure of model complexity and goodness of fit
impute_model$AIC
```

We actually see a *higher* AIC on our imputed data, rather than our eliminated predictor data. This makes intuitive sense as one of the AIC inputs is the number of predictors. When we removed our missing predictors, we improved AIC in this sense.



